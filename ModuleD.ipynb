{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhG9GKS6RYNQhbnDzoQH2v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vavvari/MAT421/blob/main/ModuleD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 Introduction**\n",
        "\n",
        "Linear algebra is a field of mathematics with many applications in data science and machine learning. The use of these concepts is present in various algorithms, and concepts like vector spaces, orthogonality, eigenvalues, matrix decomposition will be expanded upon to further our understanding of linear regression and principal component analysis."
      ],
      "metadata": {
        "id": "LoBeuUsvmvjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2 Elements of Linear Spaces**\n",
        "\n",
        "**1.2.1 Linear Combinations**\n",
        "\n",
        "A linear space is given from linear combination, which are new vectors that are made by multiplying each vector in a subset by some constant and adding the values together.\n",
        "\n",
        "Definition: A linear subspace of V is a subset U ⊆ V\n",
        "that is closed under vector addition and scalar multiplication.\n",
        "\n",
        "A span of a set of vectors is also a linear subspace, represented as -\n",
        "\n",
        "span(w1, . . . , wm) ={ m∑_j=1 α_j w_j : α1, . . . , αm ∈ R}\\"
      ],
      "metadata": {
        "id": "vFS_filOyt_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2.2 Orthogonality**\n",
        "\n",
        "The use of orthogonal bases in linear algebra can simplify our mathematical representations and reveal information about the problem we are pursuing a solution for. Given vectors u and v the norm and inner product is\n",
        "- ⟨u, v⟩ = u · v = ∑n u_i*v_i and ∥u∥ =√ ∑n (u_i) ^ 2\n",
        "\n",
        "A list of vectors is said to be orthonormal if the u_i vectors are\n",
        "pairwise orthogonal and each has a norm of 1.\n",
        "\n",
        "If U ⊆ V is a linear subspace with orthonormal basis (q1, . . . , q_m), we define the orthogonal projection of v ∈ V on U as -\n",
        "\n",
        "PU v =∑_j=1 ⟨v, q_j⟩*q_j\n",
        "\n",
        "This technique can be used to confirm optimality of vectors used and come up with the best approximation of orthonormality."
      ],
      "metadata": {
        "id": "HxmI4EbN1Yzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2.3 Gram-Schmidt Process**\n",
        "\n",
        "We use the Gram-Schmidt algorithm to find an orthonormal basis. Vectors a_i are added one by one after removing the previously used orthogonal projection.\n",
        "\n",
        "The Gram-Schmidt theorem states that with (a_1, . . . , a_m) in R_n that is  linearly independent, then there must exist an orthonormal basis (q_1, . . . , q_m) with span(a_1, . . . , a_m)."
      ],
      "metadata": {
        "id": "BNIybTyt5iYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2.4 Eigenvalues and Eigenvectors**\n",
        "\n",
        "When we have a square matrix A of real numbers, then we say that λ ∈ R is an eigenvalue of A if there exists a nonzero vector x such that Ax = λx. We then refer to the vector x here as an eigenvector, but not every matrix has eigenvalues.\n",
        "\n",
        "We can use Python code to calculate the eigenvalues and therefore eigenvectors of a given matrix:"
      ],
      "metadata": {
        "id": "qqa8ekMO6lZr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCJel5_PmYSL",
        "outputId": "736d6083-90a9-4537-d076-43d55d613c47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E-value: [17.32501772  2.28889581 -1.61391353]\n",
            "E-vector [[-0.53955301 -0.73867428 -0.32054824]\n",
            " [-0.58355188  0.65269941 -0.73792203]\n",
            " [-0.60691824 -0.16835613  0.59390227]]\n"
          ]
        }
      ],
      "source": [
        "# imports needed to calculate eigenvalue\n",
        "import numpy as np\n",
        "from numpy.linalg import eig\n",
        "\n",
        "a = np.array([[4, 4, 8],\n",
        "              [1, 6, 10],\n",
        "              [4, 6, 8]])\n",
        "\n",
        "# eigenvalue function call\n",
        "w,v=eig(a)\n",
        "print('E-value:', w)\n",
        "print('E-vector', v)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3 Linear Regression**\n",
        "\n",
        "Linear regression models depend linearly on their unknown parameters which often can make them easier to fit that non-linear models. The linear least squares problem can be solved by using QR decomposition. The Gram-Schmidt process is used to create an orthonormal basis span, which is written as QR decomposition A = QR.\n",
        "\n",
        "Python code can be used to figure out the A, Q, and R in our decomposition with ease -"
      ],
      "metadata": {
        "id": "EWKpjyg38lUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports for linear algebra functions\n",
        "import pprint\n",
        "import scipy\n",
        "import scipy.linalg\n",
        "\n",
        "# given square matrix A\n",
        "A = scipy.array([[12, -51, 4],\n",
        "                  [6, 167, -68],\n",
        "                  [-4, 24, -41]])\n",
        "# use linalg library to find decomposition\n",
        "Q, R = scipy.linalg.qr(A)\n",
        "\n",
        "print(\"A\")\n",
        "pprint.pprint(A)\n",
        "\n",
        "print(\"Q\")\n",
        "pprint.pprint(Q)\n",
        "\n",
        "print(\"R:\")\n",
        "pprint.pprint(R)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS6YpMiw-AOu",
        "outputId": "84f39a23-d9ad-4e7d-b8de-c959337a7867"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "array([[ 12, -51,   4],\n",
            "       [  6, 167, -68],\n",
            "       [ -4,  24, -41]])\n",
            "Q\n",
            "array([[-0.85714286,  0.39428571,  0.33142857],\n",
            "       [-0.42857143, -0.90285714, -0.03428571],\n",
            "       [ 0.28571429, -0.17142857,  0.94285714]])\n",
            "R:\n",
            "array([[ -14.,  -21.,   14.],\n",
            "       [   0., -175.,   70.],\n",
            "       [   0.,    0.,  -35.]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-f659c5712b76>:6: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
            "  A = scipy.array([[12, -51, 4],\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solving least squares problems can be done in a similar way, with using the least squares regression β=(A^T*A)^−1*A^T*Y to make sense of a problem in which we are unable to use matrix inverse to solve Ax = b."
      ],
      "metadata": {
        "id": "2gRi3tBN_MpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports for least squares regression\n",
        "import numpy as np\n",
        "from scipy import optimize\n",
        "\n",
        "# establish random x and y values for problem\n",
        "x = np.linspace(0, 1, 101)\n",
        "y = 1 + x + x * np.random.random(len(x))\n",
        "# assemble matrix A\n",
        "A = np.vstack([x, np.ones(len(x))]).T\n",
        "\n",
        "# make column vector for y\n",
        "y = y[:, np.newaxis]\n",
        "\n",
        "# regression\n",
        "alpha = np.dot((np.dot(np.linalg.inv(np.dot(A.T,A)),A.T)),y)\n",
        "print(alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW5ScDXf_zD3",
        "outputId": "e8ba45da-15d8-407a-81d5-bd7359f4cc48"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.68674527]\n",
            " [0.95277017]]\n"
          ]
        }
      ]
    }
  ]
}